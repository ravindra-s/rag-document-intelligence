import argparse
import logging
from pathlib import Path


import pandas as pd

from rag.config import settings
from rag.evaluation.runner import EvaluationRunner
from rag.logging_config import configure_logging


def short_model_name(model_name: str) -> str:
    """
    Convert HF model names into short, filesystem-safe tokens.
    Example:
        sentence-transformers/all-MiniLM-L6-v2 -> allminimll6v2
        google/flan-t5-base -> flant5base
    """
    return (
        model_name
        .split("/")[-1]
        .replace("-", "")
        .replace("_", "")
        .lower()
    )


def main() -> None:
    configure_logging()
    logger = logging.getLogger(__name__)

    parser = argparse.ArgumentParser(
        description="Evaluate RAG system over a list of questions (read-only input)."
    )
    parser.add_argument(
        "--input",
        required=True,
        help="Path to input CSV or Excel file containing questions",
    )
    parser.add_argument(
        "--question-col",
        default=None,
        help="Column name containing questions (default: first column)",
    )
    parser.add_argument(
        "--retrieval-k",
        type=int,
        default=8,
        help="Number of chunks to retrieve from vector store",
    )
    parser.add_argument(
        "--context-k",
        type=int,
        default=4,
        help="Number of chunks passed to the LLM as context",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=256,
        help="Maximum tokens generated by the LLM",
    )
    parser.add_argument(
        "--output",
        default=None,
        help="Optional output path (default: auto-generated per model pair)",
    )

    args = parser.parse_args()

    input_path = Path(args.input)
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")

    # Load input table (read-only)
    if input_path.suffix.lower() in {".xlsx", ".xls"}:
        df = pd.read_excel(input_path)
    else:
        df = pd.read_csv(input_path)

    if df.empty:
        raise ValueError("Input file contains no rows")

    question_col = args.question_col or df.columns[0]
    if question_col not in df.columns:
        raise ValueError(f"Question column not found: {question_col}")

    questions = df[question_col].astype(str).tolist()

    # Construct output filename (model-specific)
    embed_tag = short_model_name(settings.embedding_model_name)
    llm_tag = short_model_name(settings.llm_model_name)
    run_tag = f"{embed_tag}__{llm_tag}"

    output_path = (
        Path(args.output)
        if args.output
        else input_path.with_name(
            f"{input_path.stem}__{run_tag}{input_path.suffix}"
        )
    )

    # Column name is explicit and self-documenting
    result_col = (
        f"{settings.embedding_model_name} + {settings.llm_model_name}"
    )

    logger.info("Evaluation run")
    logger.info("Embedding model: %s", settings.embedding_model_name)
    logger.info("LLM model: %s", settings.llm_model_name)
    logger.info("Questions: %d", len(questions))
    logger.info("Output file: %s", output_path)

    runner = EvaluationRunner(
        retrieval_k=args.retrieval_k,
        context_k=args.context_k,
        max_new_tokens=args.max_new_tokens,
    )

    answers = runner.run(questions)

    # Write results to NEW file (input remains untouched)
    result_df = df.copy()
    result_df[result_col] = answers

    if output_path.suffix.lower() in {".xlsx", ".xls"}:
        result_df.to_excel(output_path, index=False)
    else:
        result_df.to_csv(output_path, index=False)

    logger.info("Evaluation complete. Results written to %s", output_path)


if __name__ == "__main__":
    main()
